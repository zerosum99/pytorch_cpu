{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 512 \n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "prep = transforms.Compose([transforms.Resize(imsize),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
    "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
    "                          ])\n",
    "postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
    "                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
    "                           ])\n",
    "postpb = transforms.Compose([transforms.ToPILImage()])\n",
    "def postp(tensor): # to clip results in the range [0,1]\n",
    "    t = postpa(tensor)\n",
    "    t[t>1] = 1    \n",
    "    t[t<0] = 0\n",
    "    img = postpb(t)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(prep(image))\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('Images/amrut1.jpg').resize((600,600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "style_img = image_loader(\"Images/vangogh_starry_night.jpg\")\n",
    "content_img = image_loader(\"Images/amrut1.jpg\")\n",
    "vgg = vgg19(pretrained=True).features\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "if is_cuda:\n",
    "    style_img = style_img.cuda()\n",
    "    content_img = content_img.cuda()\n",
    "    vgg = vgg.cuda()\n",
    "\n",
    "opt_img = Variable(content_img.data.clone(),requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = [1,6,11,20,25]\n",
    "content_layers = [21]\n",
    "loss_layers = style_layers + content_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerActivations():\n",
    "    features=[]\n",
    "    \n",
    "    def __init__(self,model,layer_nums):\n",
    "        \n",
    "        self.hooks = []\n",
    "        for layer_num in layer_nums:\n",
    "            self.hooks.append(model[layer_num].register_forward_hook(self.hook_fn))\n",
    "    \n",
    "    def hook_fn(self,module,input,output):\n",
    "        self.features.append(output)\n",
    "\n",
    "    \n",
    "    def remove(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    \n",
    "    def forward(self,input):\n",
    "        b,c,h,w = input.size()\n",
    "        features = input.view(b,c,h*w)\n",
    "        gram_matrix =  torch.bmm(features,features.transpose(1,2))\n",
    "        gram_matrix.div_(h*w)\n",
    "        return gram_matrix\n",
    "        \n",
    "class StyleLoss(nn.Module):\n",
    "    \n",
    "    def forward(self,inputs,targets):\n",
    "        out = nn.MSELoss()(GramMatrix()(inputs),targets)\n",
    "        return (out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layers(layers,img,model=None):\n",
    "    la = LayerActivations(model,layers)\n",
    "    #Clearing the cache \n",
    "    la.features = []\n",
    "    out = model(img)\n",
    "    la.remove()\n",
    "    return la.features\n",
    "\n",
    "\n",
    "content_targets = extract_layers(content_layers,content_img,model=vgg)\n",
    "content_targets = [t.detach() for t in content_targets]\n",
    "style_targets = extract_layers(style_layers,style_img,model=vgg)\n",
    "style_targets = [GramMatrix()(t).detach() for t in style_targets]\n",
    "targets = style_targets + content_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fns = [StyleLoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
    "if is_cuda:\n",
    "    loss_fns = [fn.cuda() for fn in loss_fns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are good weights settings:\n",
    "style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n",
    "content_weights = [1e0]\n",
    "weights = style_weights + content_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#run style transferu\n",
    "max_iter = 500\n",
    "show_iter = 50\n",
    "optimizer = optim.LBFGS([opt_img]);\n",
    "n_iter=[0]\n",
    "\n",
    "while n_iter[0] <= max_iter:\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = extract_layers(loss_layers,opt_img,model=vgg)\n",
    "        layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
    "        loss = sum(layer_losses)\n",
    "        loss.backward()\n",
    "        n_iter[0]+=1\n",
    "        #print loss\n",
    "        if n_iter[0]%show_iter == (show_iter-1):\n",
    "            print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.data))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 출력\n",
    "out_img_hr = postp(opt_img.data[0].cpu().squeeze())\n",
    "\n",
    "imshow(out_img_hr)\n",
    "gcf().set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
