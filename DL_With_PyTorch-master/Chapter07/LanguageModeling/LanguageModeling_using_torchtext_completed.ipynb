{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "from torchtext import data as d\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = d.Field(lower=True, batch_first=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading wikitext-2-v1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:06<00:00, 704kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n"
     ]
    }
   ],
   "source": [
    "# make splits for data\n",
    "train, valid, test = datasets.WikiText2.splits(TEXT,root='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20\n",
    "bptt_len=30\n",
    "clip = 0.25\n",
    "lr = 20\n",
    "log_interval = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217640"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(valid[0].text)//batch_size)*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217646"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].text = train[0].text[:(len(train[0].text)//batch_size)*batch_size]\n",
    "valid[0].text = valid[0].text[:(len(valid[0].text)//batch_size)*batch_size]\n",
    "test[0].text = test[0].text[:(len(valid[0].text)//batch_size)*batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217640"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x7fa8202efcf8>}\n",
      "len(train) 1\n",
      "vars(train[0]) ['<eos>', '=', 'valkyria', 'chronicles', 'iii', '=', '<eos>', '<eos>', 'senjō', 'no']\n"
     ]
    }
   ],
   "source": [
    "# print information about the data\n",
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0])['text'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 28913\n"
     ]
    }
   ],
   "source": [
    "print('len(TEXT.vocab)', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_iter, valid_iter, test_iter = d.BPTTIterator.splits((train, valid, test), batch_size=batch_size, bptt_len=bptt_len, device=0,repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,ntoken,ninp,nhid,nlayers,dropout=0.5,tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout()\n",
    "        self.encoder = nn.Embedding(ntoken,ninp)\n",
    "        self.rnn = nn.LSTM(ninp,nhid,nlayers,dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid,ntoken)\n",
    "        if tie_weights:\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange,initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange,initrange)\n",
    "        \n",
    "    def forward(self,input,hidden): \n",
    "        \n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output,hidden = self.rnn(emb,hidden)\n",
    "        output = self.drop(output)\n",
    "        s = output.size()\n",
    "        decoded = self.decoder(output.view(s[0]*s[1],s[2]))\n",
    "        return decoded.view(s[0],s[1],decoded.size(1)),hidden\n",
    "    \n",
    "    def init_hidden(self,bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return(Variable(weight.new(self.nlayers,bsz,self.nhid).zero_()),Variable(weight.new(self.nlayers,bsz,self.nhid).zero_()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217640"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_iter.dataset[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 200\n",
    "nhid=200\n",
    "nlayers=2\n",
    "dropout = 0.2\n",
    "\n",
    "ntokens = len(TEXT.vocab)\n",
    "lstm = RNNModel(ntokens, emsize, nhid,nlayers, dropout, 'store_true')\n",
    "if is_cuda:\n",
    "    lstm = lstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Tensor:\n",
    "        return h.detach().cuda()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    lstm.eval()\n",
    "    total_loss = 0   \n",
    "    hidden = lstm.init_hidden(batch_size)\n",
    "    for batch in data_source:        \n",
    "        data, targets = batch.text,batch.target.view(-1)\n",
    "        output, hidden = lstm(data.cuda(), hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        \n",
    "        if is_cuda :\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss.item()/(len(data_source.dataset[0].text)//batch_size) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainf():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    lstm.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    hidden = lstm.init_hidden(batch_size)\n",
    "    for  i,batch in enumerate(train_iter):\n",
    "        data, targets = batch.text,batch.target.view(-1)\n",
    "        if is_cuda :\n",
    "            data = data.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        lstm.zero_grad()\n",
    "        output, hidden = lstm(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm(lstm.parameters(), clip)\n",
    "        for p in lstm.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            (print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(epoch, i, len(train_iter), lr,elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss))))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taewan/anaconda3/envs/pytorch_env2/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   400/ 3481 batches | lr 20.00 | ms/batch 14.04 | loss  7.09 | ppl  1201.76\n",
      "| epoch   1 |   800/ 3481 batches | lr 20.00 | ms/batch 12.61 | loss  6.29 | ppl   536.94\n",
      "| epoch   1 |  1200/ 3481 batches | lr 20.00 | ms/batch 13.00 | loss  6.08 | ppl   436.18\n",
      "| epoch   1 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.85 | loss  5.96 | ppl   386.89\n",
      "| epoch   1 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.62 | loss  5.88 | ppl   357.64\n",
      "| epoch   1 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.61 | loss  5.80 | ppl   329.48\n",
      "| epoch   1 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.64 | loss  5.71 | ppl   300.89\n",
      "| epoch   1 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.63 | loss  5.68 | ppl   293.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 46.20s | valid loss  5.44 | valid ppl   229.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   400/ 3481 batches | lr 20.00 | ms/batch 13.46 | loss  5.63 | ppl   279.45\n",
      "| epoch   2 |   800/ 3481 batches | lr 20.00 | ms/batch 12.63 | loss  5.52 | ppl   249.53\n",
      "| epoch   2 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.61 | loss  5.51 | ppl   246.93\n",
      "| epoch   2 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.62 | loss  5.50 | ppl   244.75\n",
      "| epoch   2 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.65 | loss  5.50 | ppl   245.73\n",
      "| epoch   2 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.60 | loss  5.47 | ppl   236.80\n",
      "| epoch   2 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.62 | loss  5.40 | ppl   221.05\n",
      "| epoch   2 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.67 | loss  5.41 | ppl   223.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 45.73s | valid loss  5.23 | valid ppl   186.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   400/ 3481 batches | lr 20.00 | ms/batch 13.49 | loss  5.42 | ppl   226.28\n",
      "| epoch   3 |   800/ 3481 batches | lr 20.00 | ms/batch 12.64 | loss  5.32 | ppl   205.15\n",
      "| epoch   3 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.63 | loss  5.34 | ppl   208.00\n",
      "| epoch   3 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.64 | loss  5.35 | ppl   210.60\n",
      "| epoch   3 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.67 | loss  5.36 | ppl   213.10\n",
      "| epoch   3 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.89 | loss  5.34 | ppl   208.73\n",
      "| epoch   3 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.94 | loss  5.27 | ppl   195.17\n",
      "| epoch   3 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.68 | loss  5.29 | ppl   198.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 46.04s | valid loss  5.13 | valid ppl   169.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   400/ 3481 batches | lr 20.00 | ms/batch 13.52 | loss  5.32 | ppl   204.30\n",
      "| epoch   4 |   800/ 3481 batches | lr 20.00 | ms/batch 12.95 | loss  5.23 | ppl   185.94\n",
      "| epoch   4 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.61 | loss  5.25 | ppl   190.12\n",
      "| epoch   4 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.65 | loss  5.27 | ppl   193.53\n",
      "| epoch   4 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.62 | loss  5.29 | ppl   197.37\n",
      "| epoch   4 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.65 | loss  5.26 | ppl   193.26\n",
      "| epoch   4 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.64 | loss  5.20 | ppl   180.81\n",
      "| epoch   4 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.65 | loss  5.22 | ppl   185.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 45.98s | valid loss  5.08 | valid ppl   161.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   400/ 3481 batches | lr 20.00 | ms/batch 13.51 | loss  5.25 | ppl   190.65\n",
      "| epoch   5 |   800/ 3481 batches | lr 20.00 | ms/batch 12.79 | loss  5.16 | ppl   174.61\n",
      "| epoch   5 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.68 | loss  5.19 | ppl   179.36\n",
      "| epoch   5 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.74 | loss  5.21 | ppl   183.18\n",
      "| epoch   5 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.73 | loss  5.23 | ppl   187.69\n",
      "| epoch   5 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.68 | loss  5.22 | ppl   184.01\n",
      "| epoch   5 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.15 | ppl   171.84\n",
      "| epoch   5 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.17 | ppl   176.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 46.03s | valid loss  5.04 | valid ppl   154.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   400/ 3481 batches | lr 20.00 | ms/batch 13.51 | loss  5.21 | ppl   182.79\n",
      "| epoch   6 |   800/ 3481 batches | lr 20.00 | ms/batch 12.80 | loss  5.12 | ppl   166.66\n",
      "| epoch   6 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.72 | loss  5.15 | ppl   172.42\n",
      "| epoch   6 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.71 | loss  5.17 | ppl   175.82\n",
      "| epoch   6 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.19 | ppl   179.81\n",
      "| epoch   6 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.68 | loss  5.18 | ppl   177.54\n",
      "| epoch   6 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.78 | loss  5.11 | ppl   165.63\n",
      "| epoch   6 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.72 | loss  5.14 | ppl   170.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 46.04s | valid loss  5.01 | valid ppl   150.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   400/ 3481 batches | lr 20.00 | ms/batch 13.53 | loss  5.17 | ppl   175.30\n",
      "| epoch   7 |   800/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.08 | ppl   160.75\n",
      "| epoch   7 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.12 | ppl   166.89\n",
      "| epoch   7 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.71 | loss  5.14 | ppl   170.95\n",
      "| epoch   7 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.67 | loss  5.16 | ppl   174.44\n",
      "| epoch   7 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.81 | loss  5.15 | ppl   171.65\n",
      "| epoch   7 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.08 | ppl   160.49\n",
      "| epoch   7 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  5.11 | ppl   166.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 46.02s | valid loss  4.99 | valid ppl   147.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   400/ 3481 batches | lr 20.00 | ms/batch 13.59 | loss  5.14 | ppl   171.28\n",
      "| epoch   8 |   800/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.05 | ppl   155.59\n",
      "| epoch   8 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.67 | loss  5.09 | ppl   162.45\n",
      "| epoch   8 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.73 | loss  5.11 | ppl   166.14\n",
      "| epoch   8 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.67 | loss  5.14 | ppl   170.44\n",
      "| epoch   8 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.65 | loss  5.12 | ppl   167.68\n",
      "| epoch   8 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.68 | loss  5.05 | ppl   156.02\n",
      "| epoch   8 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.79 | loss  5.08 | ppl   161.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 46.01s | valid loss  4.97 | valid ppl   143.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   400/ 3481 batches | lr 20.00 | ms/batch 13.51 | loss  5.12 | ppl   166.77\n",
      "| epoch   9 |   800/ 3481 batches | lr 20.00 | ms/batch 12.80 | loss  5.02 | ppl   151.93\n",
      "| epoch   9 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.07 | ppl   158.73\n",
      "| epoch   9 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  5.09 | ppl   162.76\n",
      "| epoch   9 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  5.11 | ppl   166.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   9 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.66 | loss  5.10 | ppl   164.16\n",
      "| epoch   9 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.03 | ppl   152.71\n",
      "| epoch   9 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.68 | loss  5.07 | ppl   158.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 45.97s | valid loss  4.96 | valid ppl   142.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   400/ 3481 batches | lr 20.00 | ms/batch 13.64 | loss  5.10 | ppl   163.39\n",
      "| epoch  10 |   800/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.00 | ppl   148.93\n",
      "| epoch  10 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.71 | loss  5.04 | ppl   154.91\n",
      "| epoch  10 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.67 | loss  5.07 | ppl   159.11\n",
      "| epoch  10 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.68 | loss  5.10 | ppl   163.20\n",
      "| epoch  10 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.73 | loss  5.08 | ppl   161.00\n",
      "| epoch  10 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.71 | loss  5.01 | ppl   149.99\n",
      "| epoch  10 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.72 | loss  5.05 | ppl   155.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 46.04s | valid loss  4.96 | valid ppl   141.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   400/ 3481 batches | lr 20.00 | ms/batch 13.47 | loss  5.08 | ppl   161.14\n",
      "| epoch  11 |   800/ 3481 batches | lr 20.00 | ms/batch 12.67 | loss  4.98 | ppl   146.08\n",
      "| epoch  11 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.66 | loss  5.03 | ppl   152.65\n",
      "| epoch  11 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.65 | loss  5.06 | ppl   156.98\n",
      "| epoch  11 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.64 | loss  5.09 | ppl   161.70\n",
      "| epoch  11 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.80 | loss  5.07 | ppl   159.01\n",
      "| epoch  11 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.71 | loss  5.00 | ppl   148.18\n",
      "| epoch  11 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.64 | loss  5.03 | ppl   152.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 45.91s | valid loss  4.95 | valid ppl   141.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   400/ 3481 batches | lr 20.00 | ms/batch 13.52 | loss  5.07 | ppl   158.40\n",
      "| epoch  12 |   800/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  4.97 | ppl   143.55\n",
      "| epoch  12 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  5.01 | ppl   149.22\n",
      "| epoch  12 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  5.04 | ppl   154.35\n",
      "| epoch  12 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  5.07 | ppl   158.60\n",
      "| epoch  12 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.05 | ppl   156.04\n",
      "| epoch  12 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.85 | loss  4.99 | ppl   146.38\n",
      "| epoch  12 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  5.02 | ppl   151.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 46.03s | valid loss  4.94 | valid ppl   139.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   400/ 3481 batches | lr 20.00 | ms/batch 13.52 | loss  5.05 | ppl   155.97\n",
      "| epoch  13 |   800/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  4.95 | ppl   141.76\n",
      "| epoch  13 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.89 | loss  5.00 | ppl   148.16\n",
      "| epoch  13 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  5.03 | ppl   152.80\n",
      "| epoch  13 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  5.06 | ppl   157.19\n",
      "| epoch  13 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  5.04 | ppl   154.33\n",
      "| epoch  13 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.67 | loss  4.97 | ppl   143.76\n",
      "| epoch  13 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.68 | loss  5.01 | ppl   149.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 46.04s | valid loss  4.93 | valid ppl   138.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   400/ 3481 batches | lr 20.00 | ms/batch 13.52 | loss  5.04 | ppl   154.74\n",
      "| epoch  14 |   800/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  4.94 | ppl   140.09\n",
      "| epoch  14 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.68 | loss  4.99 | ppl   146.53\n",
      "| epoch  14 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.68 | loss  5.02 | ppl   151.11\n",
      "| epoch  14 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  5.05 | ppl   155.30\n",
      "| epoch  14 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.80 | loss  5.03 | ppl   153.22\n",
      "| epoch  14 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.69 | loss  4.95 | ppl   141.87\n",
      "| epoch  14 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.66 | loss  4.99 | ppl   147.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 45.98s | valid loss  4.91 | valid ppl   136.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   400/ 3481 batches | lr 20.00 | ms/batch 13.51 | loss  5.03 | ppl   152.58\n",
      "| epoch  15 |   800/ 3481 batches | lr 20.00 | ms/batch 12.70 | loss  4.93 | ppl   137.95\n",
      "| epoch  15 |  1200/ 3481 batches | lr 20.00 | ms/batch 12.68 | loss  4.98 | ppl   145.03\n",
      "| epoch  15 |  1600/ 3481 batches | lr 20.00 | ms/batch 12.67 | loss  5.01 | ppl   149.89\n",
      "| epoch  15 |  2000/ 3481 batches | lr 20.00 | ms/batch 12.67 | loss  5.03 | ppl   152.74\n",
      "| epoch  15 |  2400/ 3481 batches | lr 20.00 | ms/batch 12.65 | loss  5.02 | ppl   151.03\n",
      "| epoch  15 |  2800/ 3481 batches | lr 20.00 | ms/batch 12.80 | loss  4.94 | ppl   139.67\n",
      "| epoch  15 |  3200/ 3481 batches | lr 20.00 | ms/batch 12.73 | loss  4.99 | ppl   146.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 45.97s | valid loss  4.93 | valid ppl   138.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   400/ 3481 batches | lr 5.00 | ms/batch 13.50 | loss  5.04 | ppl   154.65\n",
      "| epoch  16 |   800/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.92 | ppl   137.10\n",
      "| epoch  16 |  1200/ 3481 batches | lr 5.00 | ms/batch 12.65 | loss  4.94 | ppl   139.85\n",
      "| epoch  16 |  1600/ 3481 batches | lr 5.00 | ms/batch 12.77 | loss  4.95 | ppl   141.24\n",
      "| epoch  16 |  2000/ 3481 batches | lr 5.00 | ms/batch 12.69 | loss  4.95 | ppl   141.32\n",
      "| epoch  16 |  2400/ 3481 batches | lr 5.00 | ms/batch 12.64 | loss  4.93 | ppl   138.00\n",
      "| epoch  16 |  2800/ 3481 batches | lr 5.00 | ms/batch 12.79 | loss  4.83 | ppl   125.32\n",
      "| epoch  16 |  3200/ 3481 batches | lr 5.00 | ms/batch 12.65 | loss  4.85 | ppl   128.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 45.95s | valid loss  4.81 | valid ppl   122.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   400/ 3481 batches | lr 5.00 | ms/batch 13.53 | loss  4.94 | ppl   139.83\n",
      "| epoch  17 |   800/ 3481 batches | lr 5.00 | ms/batch 12.69 | loss  4.84 | ppl   126.30\n",
      "| epoch  17 |  1200/ 3481 batches | lr 5.00 | ms/batch 12.67 | loss  4.88 | ppl   130.99\n",
      "| epoch  17 |  1600/ 3481 batches | lr 5.00 | ms/batch 12.67 | loss  4.90 | ppl   133.78\n",
      "| epoch  17 |  2000/ 3481 batches | lr 5.00 | ms/batch 12.77 | loss  4.91 | ppl   135.39\n",
      "| epoch  17 |  2400/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.89 | ppl   133.06\n",
      "| epoch  17 |  2800/ 3481 batches | lr 5.00 | ms/batch 12.68 | loss  4.80 | ppl   121.56\n",
      "| epoch  17 |  3200/ 3481 batches | lr 5.00 | ms/batch 12.67 | loss  4.83 | ppl   125.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 45.99s | valid loss  4.79 | valid ppl   120.23\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  18 |   400/ 3481 batches | lr 5.00 | ms/batch 13.45 | loss  4.91 | ppl   135.28\n",
      "| epoch  18 |   800/ 3481 batches | lr 5.00 | ms/batch 12.64 | loss  4.81 | ppl   122.31\n",
      "| epoch  18 |  1200/ 3481 batches | lr 5.00 | ms/batch 12.64 | loss  4.84 | ppl   127.03\n",
      "| epoch  18 |  1600/ 3481 batches | lr 5.00 | ms/batch 12.61 | loss  4.87 | ppl   130.49\n",
      "| epoch  18 |  2000/ 3481 batches | lr 5.00 | ms/batch 12.70 | loss  4.89 | ppl   132.33\n",
      "| epoch  18 |  2400/ 3481 batches | lr 5.00 | ms/batch 12.75 | loss  4.87 | ppl   130.78\n",
      "| epoch  18 |  2800/ 3481 batches | lr 5.00 | ms/batch 12.64 | loss  4.79 | ppl   119.85\n",
      "| epoch  18 |  3200/ 3481 batches | lr 5.00 | ms/batch 12.65 | loss  4.82 | ppl   123.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 45.82s | valid loss  4.79 | valid ppl   119.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   400/ 3481 batches | lr 5.00 | ms/batch 13.61 | loss  4.88 | ppl   132.20\n",
      "| epoch  19 |   800/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.79 | ppl   119.85\n",
      "| epoch  19 |  1200/ 3481 batches | lr 5.00 | ms/batch 12.74 | loss  4.82 | ppl   124.38\n",
      "| epoch  19 |  1600/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.85 | ppl   128.24\n",
      "| epoch  19 |  2000/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.87 | ppl   130.41\n",
      "| epoch  19 |  2400/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.86 | ppl   128.77\n",
      "| epoch  19 |  2800/ 3481 batches | lr 5.00 | ms/batch 12.67 | loss  4.77 | ppl   118.06\n",
      "| epoch  19 |  3200/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.81 | ppl   122.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 45.94s | valid loss  4.78 | valid ppl   118.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   400/ 3481 batches | lr 5.00 | ms/batch 13.49 | loss  4.87 | ppl   130.12\n",
      "| epoch  20 |   800/ 3481 batches | lr 5.00 | ms/batch 12.79 | loss  4.77 | ppl   117.90\n",
      "| epoch  20 |  1200/ 3481 batches | lr 5.00 | ms/batch 12.65 | loss  4.81 | ppl   122.73\n",
      "| epoch  20 |  1600/ 3481 batches | lr 5.00 | ms/batch 12.67 | loss  4.84 | ppl   126.48\n",
      "| epoch  20 |  2000/ 3481 batches | lr 5.00 | ms/batch 12.65 | loss  4.86 | ppl   128.85\n",
      "| epoch  20 |  2400/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.85 | ppl   127.29\n",
      "| epoch  20 |  2800/ 3481 batches | lr 5.00 | ms/batch 12.67 | loss  4.76 | ppl   116.67\n",
      "| epoch  20 |  3200/ 3481 batches | lr 5.00 | ms/batch 12.68 | loss  4.80 | ppl   120.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 45.93s | valid loss  4.78 | valid ppl   118.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   400/ 3481 batches | lr 5.00 | ms/batch 13.53 | loss  4.86 | ppl   128.63\n",
      "| epoch  21 |   800/ 3481 batches | lr 5.00 | ms/batch 12.69 | loss  4.75 | ppl   116.06\n",
      "| epoch  21 |  1200/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.80 | ppl   121.62\n",
      "| epoch  21 |  1600/ 3481 batches | lr 5.00 | ms/batch 12.64 | loss  4.83 | ppl   125.06\n",
      "| epoch  21 |  2000/ 3481 batches | lr 5.00 | ms/batch 12.68 | loss  4.85 | ppl   127.29\n",
      "| epoch  21 |  2400/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.83 | ppl   125.82\n",
      "| epoch  21 |  2800/ 3481 batches | lr 5.00 | ms/batch 12.67 | loss  4.75 | ppl   115.58\n",
      "| epoch  21 |  3200/ 3481 batches | lr 5.00 | ms/batch 13.06 | loss  4.79 | ppl   119.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 46.04s | valid loss  4.77 | valid ppl   118.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   400/ 3481 batches | lr 5.00 | ms/batch 13.52 | loss  4.85 | ppl   127.16\n",
      "| epoch  22 |   800/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.75 | ppl   115.10\n",
      "| epoch  22 |  1200/ 3481 batches | lr 5.00 | ms/batch 12.67 | loss  4.79 | ppl   120.18\n",
      "| epoch  22 |  1600/ 3481 batches | lr 5.00 | ms/batch 12.65 | loss  4.82 | ppl   123.98\n",
      "| epoch  22 |  2000/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.84 | ppl   126.37\n",
      "| epoch  22 |  2400/ 3481 batches | lr 5.00 | ms/batch 12.69 | loss  4.83 | ppl   125.04\n",
      "| epoch  22 |  2800/ 3481 batches | lr 5.00 | ms/batch 12.69 | loss  4.74 | ppl   114.33\n",
      "| epoch  22 |  3200/ 3481 batches | lr 5.00 | ms/batch 12.65 | loss  4.78 | ppl   118.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 45.90s | valid loss  4.76 | valid ppl   117.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   400/ 3481 batches | lr 5.00 | ms/batch 13.67 | loss  4.84 | ppl   126.37\n",
      "| epoch  23 |   800/ 3481 batches | lr 5.00 | ms/batch 12.69 | loss  4.74 | ppl   114.19\n",
      "| epoch  23 |  1200/ 3481 batches | lr 5.00 | ms/batch 12.65 | loss  4.78 | ppl   119.33\n",
      "| epoch  23 |  1600/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.81 | ppl   123.19\n",
      "| epoch  23 |  2000/ 3481 batches | lr 5.00 | ms/batch 12.68 | loss  4.83 | ppl   125.12\n",
      "| epoch  23 |  2400/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.82 | ppl   124.09\n",
      "| epoch  23 |  2800/ 3481 batches | lr 5.00 | ms/batch 12.73 | loss  4.73 | ppl   113.73\n",
      "| epoch  23 |  3200/ 3481 batches | lr 5.00 | ms/batch 12.68 | loss  4.77 | ppl   118.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 45.98s | valid loss  4.76 | valid ppl   116.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   400/ 3481 batches | lr 5.00 | ms/batch 13.53 | loss  4.83 | ppl   124.77\n",
      "| epoch  24 |   800/ 3481 batches | lr 5.00 | ms/batch 12.67 | loss  4.73 | ppl   113.21\n",
      "| epoch  24 |  1200/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.77 | ppl   117.95\n",
      "| epoch  24 |  1600/ 3481 batches | lr 5.00 | ms/batch 12.69 | loss  4.80 | ppl   121.61\n",
      "| epoch  24 |  2000/ 3481 batches | lr 5.00 | ms/batch 12.77 | loss  4.82 | ppl   124.18\n",
      "| epoch  24 |  2400/ 3481 batches | lr 5.00 | ms/batch 12.79 | loss  4.82 | ppl   123.56\n",
      "| epoch  24 |  2800/ 3481 batches | lr 5.00 | ms/batch 12.70 | loss  4.73 | ppl   112.93\n",
      "| epoch  24 |  3200/ 3481 batches | lr 5.00 | ms/batch 12.68 | loss  4.77 | ppl   117.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 46.01s | valid loss  4.76 | valid ppl   116.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   400/ 3481 batches | lr 5.00 | ms/batch 13.56 | loss  4.82 | ppl   123.42\n",
      "| epoch  25 |   800/ 3481 batches | lr 5.00 | ms/batch 12.66 | loss  4.72 | ppl   111.92\n",
      "| epoch  25 |  1200/ 3481 batches | lr 5.00 | ms/batch 12.65 | loss  4.76 | ppl   116.90\n",
      "| epoch  25 |  1600/ 3481 batches | lr 5.00 | ms/batch 12.67 | loss  4.79 | ppl   120.65\n",
      "| epoch  25 |  2000/ 3481 batches | lr 5.00 | ms/batch 12.75 | loss  4.82 | ppl   123.45\n",
      "| epoch  25 |  2400/ 3481 batches | lr 5.00 | ms/batch 12.65 | loss  4.80 | ppl   122.07\n",
      "| epoch  25 |  2800/ 3481 batches | lr 5.00 | ms/batch 12.68 | loss  4.72 | ppl   111.99\n",
      "| epoch  25 |  3200/ 3481 batches | lr 5.00 | ms/batch 12.63 | loss  4.76 | ppl   116.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 45.90s | valid loss  4.75 | valid ppl   115.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   400/ 3481 batches | lr 5.00 | ms/batch 13.50 | loss  4.81 | ppl   123.02\n",
      "| epoch  26 |   800/ 3481 batches | lr 5.00 | ms/batch 12.75 | loss  4.71 | ppl   110.96\n",
      "| epoch  26 |  1200/ 3481 batches | lr 5.00 | ms/batch 12.72 | loss  4.76 | ppl   116.44\n",
      "| epoch  26 |  1600/ 3481 batches | lr 5.00 | ms/batch 12.70 | loss  4.79 | ppl   120.08\n",
      "| epoch  26 |  2000/ 3481 batches | lr 5.00 | ms/batch 12.70 | loss  4.81 | ppl   122.78\n",
      "| epoch  26 |  2400/ 3481 batches | lr 5.00 | ms/batch 12.68 | loss  4.80 | ppl   121.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  26 |  2800/ 3481 batches | lr 5.00 | ms/batch 12.70 | loss  4.72 | ppl   111.73\n",
      "| epoch  26 |  3200/ 3481 batches | lr 5.00 | ms/batch 12.69 | loss  4.75 | ppl   116.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 45.99s | valid loss  4.75 | valid ppl   116.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   400/ 3481 batches | lr 1.25 | ms/batch 13.60 | loss  4.84 | ppl   126.02\n",
      "| epoch  27 |   800/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.74 | ppl   114.14\n",
      "| epoch  27 |  1200/ 3481 batches | lr 1.25 | ms/batch 12.67 | loss  4.78 | ppl   118.64\n",
      "| epoch  27 |  1600/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.79 | ppl   120.54\n",
      "| epoch  27 |  2000/ 3481 batches | lr 1.25 | ms/batch 12.65 | loss  4.80 | ppl   122.10\n",
      "| epoch  27 |  2400/ 3481 batches | lr 1.25 | ms/batch 12.69 | loss  4.79 | ppl   120.21\n",
      "| epoch  27 |  2800/ 3481 batches | lr 1.25 | ms/batch 12.66 | loss  4.69 | ppl   108.63\n",
      "| epoch  27 |  3200/ 3481 batches | lr 1.25 | ms/batch 12.76 | loss  4.73 | ppl   113.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 45.97s | valid loss  4.72 | valid ppl   111.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   400/ 3481 batches | lr 1.25 | ms/batch 13.50 | loss  4.81 | ppl   122.45\n",
      "| epoch  28 |   800/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.71 | ppl   110.93\n",
      "| epoch  28 |  1200/ 3481 batches | lr 1.25 | ms/batch 12.70 | loss  4.75 | ppl   116.07\n",
      "| epoch  28 |  1600/ 3481 batches | lr 1.25 | ms/batch 12.67 | loss  4.78 | ppl   118.78\n",
      "| epoch  28 |  2000/ 3481 batches | lr 1.25 | ms/batch 12.76 | loss  4.79 | ppl   120.54\n",
      "| epoch  28 |  2400/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.78 | ppl   119.27\n",
      "| epoch  28 |  2800/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.68 | ppl   108.00\n",
      "| epoch  28 |  3200/ 3481 batches | lr 1.25 | ms/batch 12.65 | loss  4.72 | ppl   112.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 45.96s | valid loss  4.71 | valid ppl   111.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   400/ 3481 batches | lr 1.25 | ms/batch 13.60 | loss  4.80 | ppl   121.62\n",
      "| epoch  29 |   800/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.70 | ppl   109.72\n",
      "| epoch  29 |  1200/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.74 | ppl   114.79\n",
      "| epoch  29 |  1600/ 3481 batches | lr 1.25 | ms/batch 12.67 | loss  4.77 | ppl   118.03\n",
      "| epoch  29 |  2000/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.79 | ppl   119.77\n",
      "| epoch  29 |  2400/ 3481 batches | lr 1.25 | ms/batch 12.66 | loss  4.78 | ppl   118.98\n",
      "| epoch  29 |  2800/ 3481 batches | lr 1.25 | ms/batch 12.67 | loss  4.68 | ppl   107.51\n",
      "| epoch  29 |  3200/ 3481 batches | lr 1.25 | ms/batch 12.78 | loss  4.72 | ppl   112.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 45.98s | valid loss  4.71 | valid ppl   111.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   400/ 3481 batches | lr 1.25 | ms/batch 13.50 | loss  4.79 | ppl   119.94\n",
      "| epoch  30 |   800/ 3481 batches | lr 1.25 | ms/batch 12.72 | loss  4.69 | ppl   109.10\n",
      "| epoch  30 |  1200/ 3481 batches | lr 1.25 | ms/batch 12.69 | loss  4.74 | ppl   114.12\n",
      "| epoch  30 |  1600/ 3481 batches | lr 1.25 | ms/batch 12.66 | loss  4.76 | ppl   117.30\n",
      "| epoch  30 |  2000/ 3481 batches | lr 1.25 | ms/batch 12.66 | loss  4.78 | ppl   119.24\n",
      "| epoch  30 |  2400/ 3481 batches | lr 1.25 | ms/batch 12.67 | loss  4.77 | ppl   118.42\n",
      "| epoch  30 |  2800/ 3481 batches | lr 1.25 | ms/batch 12.79 | loss  4.68 | ppl   107.53\n",
      "| epoch  30 |  3200/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.72 | ppl   111.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 45.95s | valid loss  4.71 | valid ppl   110.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   400/ 3481 batches | lr 1.25 | ms/batch 13.52 | loss  4.79 | ppl   120.31\n",
      "| epoch  31 |   800/ 3481 batches | lr 1.25 | ms/batch 12.72 | loss  4.69 | ppl   109.17\n",
      "| epoch  31 |  1200/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.73 | ppl   113.83\n",
      "| epoch  31 |  1600/ 3481 batches | lr 1.25 | ms/batch 12.70 | loss  4.76 | ppl   116.60\n",
      "| epoch  31 |  2000/ 3481 batches | lr 1.25 | ms/batch 12.81 | loss  4.78 | ppl   119.04\n",
      "| epoch  31 |  2400/ 3481 batches | lr 1.25 | ms/batch 12.70 | loss  4.77 | ppl   118.10\n",
      "| epoch  31 |  2800/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.68 | ppl   107.28\n",
      "| epoch  31 |  3200/ 3481 batches | lr 1.25 | ms/batch 12.72 | loss  4.72 | ppl   111.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 46.02s | valid loss  4.71 | valid ppl   110.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |   400/ 3481 batches | lr 1.25 | ms/batch 13.46 | loss  4.78 | ppl   119.45\n",
      "| epoch  32 |   800/ 3481 batches | lr 1.25 | ms/batch 12.66 | loss  4.68 | ppl   108.01\n",
      "| epoch  32 |  1200/ 3481 batches | lr 1.25 | ms/batch 12.62 | loss  4.73 | ppl   113.08\n",
      "| epoch  32 |  1600/ 3481 batches | lr 1.25 | ms/batch 12.69 | loss  4.76 | ppl   116.18\n",
      "| epoch  32 |  2000/ 3481 batches | lr 1.25 | ms/batch 12.77 | loss  4.78 | ppl   119.16\n",
      "| epoch  32 |  2400/ 3481 batches | lr 1.25 | ms/batch 12.63 | loss  4.77 | ppl   117.69\n",
      "| epoch  32 |  2800/ 3481 batches | lr 1.25 | ms/batch 12.64 | loss  4.67 | ppl   106.87\n",
      "| epoch  32 |  3200/ 3481 batches | lr 1.25 | ms/batch 12.64 | loss  4.72 | ppl   111.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 45.85s | valid loss  4.71 | valid ppl   110.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |   400/ 3481 batches | lr 1.25 | ms/batch 13.53 | loss  4.78 | ppl   119.29\n",
      "| epoch  33 |   800/ 3481 batches | lr 1.25 | ms/batch 12.71 | loss  4.68 | ppl   107.97\n",
      "| epoch  33 |  1200/ 3481 batches | lr 1.25 | ms/batch 12.81 | loss  4.72 | ppl   112.50\n",
      "| epoch  33 |  1600/ 3481 batches | lr 1.25 | ms/batch 12.70 | loss  4.75 | ppl   116.01\n",
      "| epoch  33 |  2000/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.77 | ppl   118.38\n",
      "| epoch  33 |  2400/ 3481 batches | lr 1.25 | ms/batch 12.66 | loss  4.76 | ppl   117.21\n",
      "| epoch  33 |  2800/ 3481 batches | lr 1.25 | ms/batch 12.69 | loss  4.67 | ppl   106.62\n",
      "| epoch  33 |  3200/ 3481 batches | lr 1.25 | ms/batch 12.71 | loss  4.71 | ppl   111.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 46.00s | valid loss  4.70 | valid ppl   110.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   400/ 3481 batches | lr 1.25 | ms/batch 13.52 | loss  4.77 | ppl   118.44\n",
      "| epoch  34 |   800/ 3481 batches | lr 1.25 | ms/batch 12.89 | loss  4.67 | ppl   107.03\n",
      "| epoch  34 |  1200/ 3481 batches | lr 1.25 | ms/batch 12.69 | loss  4.72 | ppl   112.49\n",
      "| epoch  34 |  1600/ 3481 batches | lr 1.25 | ms/batch 12.68 | loss  4.75 | ppl   115.65\n",
      "| epoch  34 |  2000/ 3481 batches | lr 1.25 | ms/batch 12.69 | loss  4.77 | ppl   117.46\n",
      "| epoch  34 |  2400/ 3481 batches | lr 1.25 | ms/batch 12.69 | loss  4.76 | ppl   117.05\n",
      "| epoch  34 |  2800/ 3481 batches | lr 1.25 | ms/batch 12.69 | loss  4.67 | ppl   106.48\n",
      "| epoch  34 |  3200/ 3481 batches | lr 1.25 | ms/batch 12.69 | loss  4.71 | ppl   111.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 46.03s | valid loss  4.71 | valid ppl   110.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |   400/ 3481 batches | lr 0.31 | ms/batch 13.53 | loss  4.80 | ppl   121.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  35 |   800/ 3481 batches | lr 0.31 | ms/batch 12.68 | loss  4.70 | ppl   110.14\n",
      "| epoch  35 |  1200/ 3481 batches | lr 0.31 | ms/batch 12.70 | loss  4.74 | ppl   114.62\n",
      "| epoch  35 |  1600/ 3481 batches | lr 0.31 | ms/batch 12.68 | loss  4.76 | ppl   117.16\n",
      "| epoch  35 |  2000/ 3481 batches | lr 0.31 | ms/batch 12.69 | loss  4.77 | ppl   118.46\n",
      "| epoch  35 |  2400/ 3481 batches | lr 0.31 | ms/batch 12.79 | loss  4.76 | ppl   117.19\n",
      "| epoch  35 |  2800/ 3481 batches | lr 0.31 | ms/batch 12.70 | loss  4.66 | ppl   106.09\n",
      "| epoch  35 |  3200/ 3481 batches | lr 0.31 | ms/batch 12.69 | loss  4.71 | ppl   110.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 46.03s | valid loss  4.69 | valid ppl   109.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |   400/ 3481 batches | lr 0.31 | ms/batch 13.54 | loss  4.78 | ppl   119.53\n",
      "| epoch  36 |   800/ 3481 batches | lr 0.31 | ms/batch 12.68 | loss  4.69 | ppl   108.90\n",
      "| epoch  36 |  1200/ 3481 batches | lr 0.31 | ms/batch 12.69 | loss  4.73 | ppl   113.61\n",
      "| epoch  36 |  1600/ 3481 batches | lr 0.31 | ms/batch 12.81 | loss  4.76 | ppl   116.33\n",
      "| epoch  36 |  2000/ 3481 batches | lr 0.31 | ms/batch 12.68 | loss  4.77 | ppl   117.88\n",
      "| epoch  36 |  2400/ 3481 batches | lr 0.31 | ms/batch 12.68 | loss  4.76 | ppl   116.98\n",
      "| epoch  36 |  2800/ 3481 batches | lr 0.31 | ms/batch 12.74 | loss  4.67 | ppl   106.34\n",
      "| epoch  36 |  3200/ 3481 batches | lr 0.31 | ms/batch 12.81 | loss  4.70 | ppl   110.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 46.06s | valid loss  4.69 | valid ppl   109.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |   400/ 3481 batches | lr 0.31 | ms/batch 13.53 | loss  4.78 | ppl   119.35\n",
      "| epoch  37 |   800/ 3481 batches | lr 0.31 | ms/batch 12.70 | loss  4.68 | ppl   108.25\n",
      "| epoch  37 |  1200/ 3481 batches | lr 0.31 | ms/batch 12.67 | loss  4.73 | ppl   112.86\n",
      "| epoch  37 |  1600/ 3481 batches | lr 0.31 | ms/batch 12.71 | loss  4.76 | ppl   116.37\n",
      "| epoch  37 |  2000/ 3481 batches | lr 0.31 | ms/batch 12.70 | loss  4.77 | ppl   118.02\n",
      "| epoch  37 |  2400/ 3481 batches | lr 0.31 | ms/batch 12.66 | loss  4.76 | ppl   116.68\n",
      "| epoch  37 |  2800/ 3481 batches | lr 0.31 | ms/batch 12.70 | loss  4.66 | ppl   106.09\n",
      "| epoch  37 |  3200/ 3481 batches | lr 0.31 | ms/batch 12.72 | loss  4.71 | ppl   110.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 45.97s | valid loss  4.69 | valid ppl   108.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   400/ 3481 batches | lr 0.31 | ms/batch 13.55 | loss  4.78 | ppl   119.59\n",
      "| epoch  38 |   800/ 3481 batches | lr 0.31 | ms/batch 12.79 | loss  4.68 | ppl   108.11\n",
      "| epoch  38 |  1200/ 3481 batches | lr 0.31 | ms/batch 12.67 | loss  4.73 | ppl   113.17\n",
      "| epoch  38 |  1600/ 3481 batches | lr 0.31 | ms/batch 12.71 | loss  4.75 | ppl   115.89\n",
      "| epoch  38 |  2000/ 3481 batches | lr 0.31 | ms/batch 12.68 | loss  4.77 | ppl   117.91\n",
      "| epoch  38 |  2400/ 3481 batches | lr 0.31 | ms/batch 12.70 | loss  4.76 | ppl   116.77\n",
      "| epoch  38 |  2800/ 3481 batches | lr 0.31 | ms/batch 12.72 | loss  4.66 | ppl   106.00\n",
      "| epoch  38 |  3200/ 3481 batches | lr 0.31 | ms/batch 12.71 | loss  4.71 | ppl   110.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 46.01s | valid loss  4.69 | valid ppl   108.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   400/ 3481 batches | lr 0.08 | ms/batch 13.50 | loss  4.79 | ppl   120.31\n",
      "| epoch  39 |   800/ 3481 batches | lr 0.08 | ms/batch 12.67 | loss  4.70 | ppl   109.53\n",
      "| epoch  39 |  1200/ 3481 batches | lr 0.08 | ms/batch 12.78 | loss  4.73 | ppl   113.56\n",
      "| epoch  39 |  1600/ 3481 batches | lr 0.08 | ms/batch 12.70 | loss  4.76 | ppl   116.64\n",
      "| epoch  39 |  2000/ 3481 batches | lr 0.08 | ms/batch 12.65 | loss  4.77 | ppl   117.91\n",
      "| epoch  39 |  2400/ 3481 batches | lr 0.08 | ms/batch 12.65 | loss  4.76 | ppl   116.73\n",
      "| epoch  39 |  2800/ 3481 batches | lr 0.08 | ms/batch 12.66 | loss  4.66 | ppl   105.62\n",
      "| epoch  39 |  3200/ 3481 batches | lr 0.08 | ms/batch 12.75 | loss  4.70 | ppl   110.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 45.93s | valid loss  4.69 | valid ppl   108.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |   400/ 3481 batches | lr 0.08 | ms/batch 13.53 | loss  4.78 | ppl   119.54\n",
      "| epoch  40 |   800/ 3481 batches | lr 0.08 | ms/batch 12.81 | loss  4.69 | ppl   108.91\n",
      "| epoch  40 |  1200/ 3481 batches | lr 0.08 | ms/batch 12.70 | loss  4.73 | ppl   113.60\n",
      "| epoch  40 |  1600/ 3481 batches | lr 0.08 | ms/batch 12.70 | loss  4.75 | ppl   116.04\n",
      "| epoch  40 |  2000/ 3481 batches | lr 0.08 | ms/batch 12.67 | loss  4.77 | ppl   117.67\n",
      "| epoch  40 |  2400/ 3481 batches | lr 0.08 | ms/batch 12.69 | loss  4.76 | ppl   116.49\n",
      "| epoch  40 |  2800/ 3481 batches | lr 0.08 | ms/batch 12.67 | loss  4.66 | ppl   105.71\n",
      "| epoch  40 |  3200/ 3481 batches | lr 0.08 | ms/batch 12.70 | loss  4.70 | ppl   110.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 45.99s | valid loss  4.69 | valid ppl   108.69\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "epochs = 40\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    trainf()\n",
    "    val_loss = evaluate(valid_iter)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                   val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
